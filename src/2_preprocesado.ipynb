{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preprocesado de texto"
      ],
      "metadata": {
        "id": "AjemRS9pIBwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importar librerías necesarias\n",
        "\n",
        "Cargar los archivos previamente en el entorno de Google Colab."
      ],
      "metadata": {
        "id": "Mm58W5lSEy7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hly5_4HNFjYZ",
        "outputId": "5d542056-a745-4680-bbf9-a6c086b3c742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (4.12.3)\n",
            "Collecting contractions (from -r requirements.txt (line 2))\n",
            "  Using cached contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.3.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (5.5.6)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (7.34.0)\n",
            "Collecting jellyfish (from -r requirements.txt (line 6))\n",
            "  Using cached jellyfish-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (4.9.4)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (3.7.1)\n",
            "Collecting negspacy (from -r requirements.txt (line 10))\n",
            "  Using cached negspacy-1.0.4.tar.gz (13 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (3.8.1)\n",
            "Collecting num2words (from -r requirements.txt (line 12))\n",
            "  Using cached num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.25.2)\n",
            "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (1.5.3)\n",
            "Collecting plotly_express (from -r requirements.txt (line 15))\n",
            "  Using cached plotly_express-0.4.1-py2.py3-none-any.whl (2.9 kB)\n",
            "Collecting pyDAWG (from -r requirements.txt (line 16))\n",
            "  Using cached pyDAWG-1.0.1.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyLDAvis (from -r requirements.txt (line 17))\n",
            "  Using cached pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (3.7.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (1.11.4)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (0.13.1)\n",
            "Collecting sklearn_crfsuite (from -r requirements.txt (line 22))\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting stop_words (from -r requirements.txt (line 23))\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (2.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (4.66.2)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 26)) (1.9.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r requirements.txt (line 14)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r requirements.txt (line 14)) (2023.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->-r requirements.txt (line 1)) (2.5)\n",
            "Collecting textsearch>=0.0.21 (from contractions->-r requirements.txt (line 2))\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->-r requirements.txt (line 3)) (6.4.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 4)) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 4)) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 4)) (6.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 5)) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->-r requirements.txt (line 5))\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 5)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 5)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 5)) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 5)) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 5)) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 5)) (4.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 9)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 9)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 9)) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 9)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 9)) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 9)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 9)) (3.1.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 11)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 11)) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 11)) (2023.12.25)\n",
            "Collecting docopt>=0.6.2 (from num2words->-r requirements.txt (line 12))\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: plotly>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from plotly_express->-r requirements.txt (line 15)) (5.15.0)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from plotly_express->-r requirements.txt (line 15)) (0.14.1)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.10/dist-packages (from plotly_express->-r requirements.txt (line 15)) (0.5.6)\n",
            "INFO: pip is looking at multiple versions of pyldavis to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pyLDAvis (from -r requirements.txt (line 17))\n",
            "  Downloading pyLDAvis-3.4.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis->-r requirements.txt (line 17)) (3.1.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis->-r requirements.txt (line 17)) (2.9.0)\n",
            "Collecting funcy (from pyLDAvis->-r requirements.txt (line 17))\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 18)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 18)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 18)) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 18)) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 18)) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 18)) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 18)) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 18)) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 18)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 18)) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 18)) (0.9.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 18)) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 18)) (2.6.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 18)) (3.3.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 19)) (3.3.0)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn_crfsuite->-r requirements.txt (line 22))\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite->-r requirements.txt (line 22)) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite->-r requirements.txt (line 22)) (0.9.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 24)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 24)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 24)) (24.3.7)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 24)) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 24)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 24)) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 24)) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 24)) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 24)) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 24)) (3.20.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 24)) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 24)) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 24)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 24)) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 24)) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 24)) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 24)) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 24)) (0.43.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->-r requirements.txt (line 5)) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.1.0->plotly_express->-r requirements.txt (line 15)) (8.2.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r requirements.txt (line 5)) (0.2.13)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 18)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 18)) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 18)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 18)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 18)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 18)) (2024.2.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 24)) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 24)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 24)) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 24)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 24)) (3.0.1)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions->-r requirements.txt (line 2))\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions->-r requirements.txt (line 2))\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->-r requirements.txt (line 18)) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->-r requirements.txt (line 18)) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy->-r requirements.txt (line 18)) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis->-r requirements.txt (line 17)) (2.1.5)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 4)) (5.7.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 4)) (23.2.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 24)) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 24)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 24)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 24)) (1.4.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.0->jupyter-client->ipykernel->-r requirements.txt (line 4)) (4.2.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 24)) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->-r requirements.txt (line 24)) (3.2.2)\n",
            "Building wheels for collected packages: negspacy, pyDAWG, stop_words, docopt\n",
            "  Building wheel for negspacy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for negspacy: filename=negspacy-1.0.4-py3-none-any.whl size=12535 sha256=42de84794a63abeca676d84efaf30fbf4022bb4277dafef203d9e1d98e455c49\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/31/f0/3a217aaedf320e4df653347cd6538f3648263b864c8e140853\n",
            "  Building wheel for pyDAWG (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyDAWG: filename=pyDAWG-1.0.1-cp310-cp310-linux_x86_64.whl size=62204 sha256=ba734786a0c47a94d00caad980cbbc6f1b0ffc26062dc5948b44ac8c30158a6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/5a/ec/7dfb31587fa2fee403d96efdb4ba07a1f85fb1aedf74c97da5\n",
            "  Building wheel for stop_words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop_words: filename=stop_words-2018.7.23-py3-none-any.whl size=32895 sha256=da002fd1cd998426a7a7cffaf23c6ef4f239bf8b4707132417e0ad638ab3f6f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/1a/23/f12552a50cb09bcc1694a5ebb6c2cd5f2a0311de2b8c3d9a89\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=f77afccce54f5d1c242f8463e7e4711557793af7f97e3f7bbe3fcab75c5761d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built negspacy pyDAWG stop_words docopt\n",
            "Installing collected packages: stop_words, python-crfsuite, pyDAWG, funcy, docopt, sklearn_crfsuite, pyahocorasick, num2words, jellyfish, jedi, anyascii, textsearch, pyLDAvis, contractions, plotly_express, negspacy\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 docopt-0.6.2 funcy-2.0 jedi-0.19.1 jellyfish-1.0.3 negspacy-1.0.4 num2words-0.5.13 plotly_express-0.4.1 pyDAWG-1.0.1 pyLDAvis-3.4.0 pyahocorasick-2.1.0 python-crfsuite-0.9.10 sklearn_crfsuite-0.3.6 stop_words-2018.7.23 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import *\n",
        "from preprocessing import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf8-HjJUJ7PI",
        "outputId": "25a59914-4203-44d3-b759-e0f3c17cd3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cargar Datos desde Archivo CSV\n",
        "\n",
        "Debemos subir previamente el archivo al entorno de Google Colab o cargarlo desde Google Drive.\n",
        "\n",
        "#### Google Drive"
      ],
      "metadata": {
        "id": "BRMQiV3D3Qe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df = load_data_drive('Colab Notebooks/Sports_and_Outdoors/sample_data.csv')"
      ],
      "metadata": {
        "id": "_TFgp2C23Nkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09733399-cd87-4f8b-dc86-f5656b13c463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive ya está montado.\n",
            "Archivo cargado exitosamente desde: /content/drive/My Drive/Colab Notebooks/Sports_and_Outdoors/sample_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Archivos locales"
      ],
      "metadata": {
        "id": "4bCdv1IZ6M2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip sample_data_balanced_complete.csv.gz  # Descomprimimos el archivo"
      ],
      "metadata": {
        "id": "RvJzkdnL2kNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_data('sample_data_balanced_complete.csv')"
      ],
      "metadata": {
        "id": "I82X01bR4pfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeplndyRxCBG",
        "outputId": "551c32ae-cd47-4c5b-a7e8-d2b91f51c405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['rating', 'title', 'text', 'images', 'asin', 'parent_asin', 'user_id',\n",
            "       'timestamp', 'helpful_vote', 'verified_purchase', 'sentiment',\n",
            "       'cleaned_text', 'cleaned_text_exclude_numbers', 'text_length'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['text'].head()) # Ver resultados"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUuI8Muy515C",
        "outputId": "da2c4e75-e8e7-48a5-ec63-05f1903b8e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    The travel mug was OK, not bad but the coffee ...\n",
            "1    I'm terrible with planks so I was terrible at ...\n",
            "2    Drawstring immediately ripped inches of stitch...\n",
            "3    The black face is cracking and falling off aft...\n",
            "4           Leaves white stuff in your hair. Not good!\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFM-JnWd4UEd",
        "outputId": "be5ec3ef-9faf-4f19-93ae-a4fe169869c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 14 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   rating                        50000 non-null  float64\n",
            " 1   title                         49999 non-null  object \n",
            " 2   text                          49998 non-null  object \n",
            " 3   images                        50000 non-null  object \n",
            " 4   asin                          50000 non-null  object \n",
            " 5   parent_asin                   50000 non-null  object \n",
            " 6   user_id                       50000 non-null  object \n",
            " 7   timestamp                     50000 non-null  int64  \n",
            " 8   helpful_vote                  50000 non-null  int64  \n",
            " 9   verified_purchase             50000 non-null  bool   \n",
            " 10  sentiment                     50000 non-null  object \n",
            " 11  cleaned_text                  49920 non-null  object \n",
            " 12  cleaned_text_exclude_numbers  49918 non-null  object \n",
            " 13  text_length                   50000 non-null  int64  \n",
            "dtypes: bool(1), float64(1), int64(3), object(9)\n",
            "memory usage: 5.0+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminamos valores NaN\n",
        "df['text'] = df['text'].fillna('')\n",
        "df['title'] = df['title'].fillna('')"
      ],
      "metadata": {
        "id": "1MFQ5MFF4cE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aunque se realizó un pequeño preprocesamiento en los datos en la fase anterior, no se tomará en cuenta. A partir de ahora, se trabajará en la columna `'text'` desde cero (etapa de preprocesamiento)."
      ],
      "metadata": {
        "id": "ISEHzPWr3QaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesado"
      ],
      "metadata": {
        "id": "H0DNGP8_aXJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline con SpaCy"
      ],
      "metadata": {
        "id": "31nIqOugOsZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionamos una muestra de reseñas para el análisis\n",
        "sample_reviews = df['text'].sample(n=40)  # Podemos ajustar 'n' al número de reseñas que queremos explorar\n",
        "\n",
        "# Función para identificar y mostrar los tipos de entidades en las reseñas\n",
        "def identify_entity_types(reviews):\n",
        "    entity_types = set()\n",
        "\n",
        "    for review in reviews:\n",
        "        doc = nlp(review)\n",
        "        for ent in doc.ents:\n",
        "            entity_types.add((ent.text, ent.label_))\n",
        "\n",
        "    return entity_types\n",
        "\n",
        "# Identificamos los tipos de entidades en la muestra de reseñas\n",
        "entity_types_in_sample = identify_entity_types(sample_reviews)\n",
        "\n",
        "# Imprimimos los tipos de entidades encontrados\n",
        "for text, label in entity_types_in_sample:\n",
        "    print(f\"Texto: {text}, Tipo de Entidad: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eVxLLS9OqgM",
        "outputId": "24ae6557-2e27-4aac-806d-2e97453a8034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto: 168mm, Tipo de Entidad: QUANTITY\n",
            "Texto: first, Tipo de Entidad: ORDINAL\n",
            "Texto: 29, Tipo de Entidad: CARDINAL\n",
            "Texto: half, Tipo de Entidad: CARDINAL\n",
            "Texto: BOTH, Tipo de Entidad: NORP\n",
            "Texto: these days, Tipo de Entidad: DATE\n",
            "Texto: at least 4 weeks, Tipo de Entidad: DATE\n",
            "Texto: Time, Tipo de Entidad: ORG\n",
            "Texto: month, Tipo de Entidad: DATE\n",
            "Texto: the 90 day, Tipo de Entidad: DATE\n",
            "Texto: today, Tipo de Entidad: DATE\n",
            "Texto: 25%, Tipo de Entidad: PERCENT\n",
            "Texto: daily, Tipo de Entidad: DATE\n",
            "Texto: winter, Tipo de Entidad: DATE\n",
            "Texto: 550, Tipo de Entidad: CARDINAL\n",
            "Texto: 8 years, Tipo de Entidad: DATE\n",
            "Texto: 6 inches, Tipo de Entidad: QUANTITY\n",
            "Texto: Iron Bull Strength, Tipo de Entidad: ORG\n",
            "Texto: the Fat Grips Pro, Tipo de Entidad: EVENT\n",
            "Texto: Alpha Grips, Tipo de Entidad: PERSON\n",
            "Texto: 10 or 15 minutes, Tipo de Entidad: TIME\n",
            "Texto: 1, Tipo de Entidad: CARDINAL\n",
            "Texto: 4, Tipo de Entidad: CARDINAL\n",
            "Texto: the summer, Tipo de Entidad: DATE\n",
            "Texto: Great Customer Service, Tipo de Entidad: ORG\n",
            "Texto: 6 weeks, Tipo de Entidad: DATE\n",
            "Texto: the Fat Gripz Ones, Tipo de Entidad: EVENT\n",
            "Texto: four, Tipo de Entidad: CARDINAL\n",
            "Texto: 1.25, Tipo de Entidad: CARDINAL\n",
            "Texto: 34;long, Tipo de Entidad: MONEY\n",
            "Texto: two, Tipo de Entidad: CARDINAL\n",
            "Texto: the next day, Tipo de Entidad: DATE\n",
            "Texto: Peloton, Tipo de Entidad: ORG\n",
            "Texto: about three feet, Tipo de Entidad: QUANTITY\n",
            "Texto: Velcro, Tipo de Entidad: ORG\n",
            "Texto: California King, Tipo de Entidad: ORG\n",
            "Texto: CAP, Tipo de Entidad: ORG\n",
            "Texto: hose, Tipo de Entidad: PERSON\n",
            "Texto: between 1.1 and 1.25, Tipo de Entidad: CARDINAL\n",
            "Texto: GREAT, Tipo de Entidad: ORG\n",
            "Texto: Elbow, Tipo de Entidad: PERSON\n",
            "Texto: 2.25, Tipo de Entidad: CARDINAL\n",
            "Texto: 2, Tipo de Entidad: CARDINAL\n",
            "Texto: 2.2\").<br, Tipo de Entidad: CARDINAL\n",
            "Texto: a month, Tipo de Entidad: DATE\n",
            "Texto: IronMaster, Tipo de Entidad: PRODUCT\n",
            "Texto: 25lbs - where, Tipo de Entidad: DATE\n",
            "Texto: 6'4, Tipo de Entidad: DATE\n",
            "Texto: Alaska, Tipo de Entidad: GPE\n",
            "Texto: FIRST, Tipo de Entidad: ORDINAL\n",
            "Texto: BOTH, Tipo de Entidad: ORG\n",
            "Texto: Fitbit, Tipo de Entidad: PRODUCT\n",
            "Texto: a year, Tipo de Entidad: DATE\n",
            "Texto: about 1.1, Tipo de Entidad: CARDINAL\n",
            "Texto: #34;a, Tipo de Entidad: CARDINAL\n",
            "Texto: multi-day, Tipo de Entidad: DATE\n",
            "Texto: 5th, Tipo de Entidad: ORDINAL\n",
            "Texto: 250, Tipo de Entidad: CARDINAL\n",
            "Texto: One, Tipo de Entidad: CARDINAL\n",
            "Texto: Fat Gripz Pro, Tipo de Entidad: ORG\n",
            "Texto: Fat Gripz One, Tipo de Entidad: EVENT\n",
            "Texto: 10 year old, Tipo de Entidad: DATE\n",
            "Texto: 6 5/8, Tipo de Entidad: CARDINAL\n",
            "Texto: 1.75, Tipo de Entidad: CARDINAL\n",
            "Texto: 83.2kg, Tipo de Entidad: QUANTITY\n",
            "Texto: day, Tipo de Entidad: DATE\n",
            "Texto: over 5 feet, Tipo de Entidad: QUANTITY\n",
            "Texto: Fat Gripz Pros, Tipo de Entidad: ORG\n",
            "Texto: October 2022, Tipo de Entidad: DATE\n",
            "Texto: tore off first day, Tipo de Entidad: DATE\n",
            "Texto: January 2023, Tipo de Entidad: DATE\n",
            "Texto: a summer night, Tipo de Entidad: TIME\n",
            "Texto: Thick Bar Grips, Tipo de Entidad: PERSON\n",
            "Texto: 30, Tipo de Entidad: CARDINAL\n",
            "Texto: one, Tipo de Entidad: CARDINAL\n",
            "Texto: ONE, Tipo de Entidad: CARDINAL\n",
            "Texto: tonight, Tipo de Entidad: TIME\n",
            "Texto: the Iron Bull Strength, Tipo de Entidad: ORG\n",
            "Texto: 23, Tipo de Entidad: CARDINAL\n",
            "Texto: Golfer, Tipo de Entidad: PERSON\n",
            "Texto: almost 50%, Tipo de Entidad: PERCENT\n",
            "Texto: The Fat Grips One, Tipo de Entidad: EVENT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline 1: Preprocesamiento con spaCy"
      ],
      "metadata": {
        "id": "WMQtF_khsB4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de uso de la función (prueba)\n",
        "sample_review = \"I'm not loving the new tent I bought. It isn't extremely good!\"\n",
        "cleaned_review = clean_text_spacy(sample_review)\n",
        "print(cleaned_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5qMdrXAMdbb",
        "outputId": "7d357745-a099-4a00-cc99-f4efcf9db717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "love new tent buy extremely good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este primer Pipeline, empleamos spaCy y NegSpacy para el preprocesamiento del texto, con el fin de mejorar la detección de negaciones en el contexto de entidades nombradas. A pesar de que este enfoque enriquece el análisis, su limitación reside en no modificar directamente el texto para reflejar las negaciones. Esta consideración nos lleva a plantear otros pipelines."
      ],
      "metadata": {
        "id": "hbXf3o_ZUxhM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipelines con NLTK"
      ],
      "metadata": {
        "id": "WTooy3xDRo8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline 2: Limpieza de Texto Básica con NLTK"
      ],
      "metadata": {
        "id": "-7EOCzUmsH6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(clean_text(\"I'm not loving the new tent I bought. It's extremely good!\")) # Ejemplo de uso de la función (prueba)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHX9McjpG9hV",
        "outputId": "95a5978d-8785-4f3c-95c1-67c2da1ffe82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "love new tent bought extrem good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uso de la función de limpieza en el DataFrame\n",
        "df['cleaned_text'] = df['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "ihChKJeUG6DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['text'].head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9x3tgVIcEaM",
        "outputId": "021eec72-e1b5-40ca-f766-88536f7e3958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0     The travel mug was OK, not bad but the coffee ...\n",
            "1     I'm terrible with planks so I was terrible at ...\n",
            "2     Drawstring immediately ripped inches of stitch...\n",
            "3     The black face is cracking and falling off aft...\n",
            "4            Leaves white stuff in your hair. Not good!\n",
            "5     Too large and not for little girls. More for a...\n",
            "6     Update... after charging approximately 3 times...\n",
            "7     you may be fooled by soft inner padding, but t...\n",
            "8     I purchased two of these.  I put them on my do...\n",
            "9     Missing hardware and pieces didn't fit togethe...\n",
            "10    Look, this is probably my fault, but I thought...\n",
            "11    I wanted to love this... a few of us bought th...\n",
            "12    just so not what i was thinking it was gonna b...\n",
            "13    Bought this as a small gift for a Scouting Fri...\n",
            "14    The water bottle silicone coat smells strongly...\n",
            "15                                     Not true to size\n",
            "16    This is advertised as one full quart, but by w...\n",
            "17    I am posting a \"reserved\" review...After the e...\n",
            "18    We bought this for a 5 year old that swims lik...\n",
            "19    What a waste! I was so excited to buy a “high ...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['cleaned_text'].head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBTapqDpHQ_I",
        "outputId": "bf004773-366c-4faa-afc6-89417e2cd628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0     travel mug wa ok bad coffe mug team emblem wa ...\n",
            "1     terribl plank wa terribl thi mayb ab need lot ...\n",
            "2           drawstr immedi rip inch stitch produci unus\n",
            "3            black face crack fall two day poor qualiti\n",
            "4                            leaf white stuff hair good\n",
            "5                      larg littl girl adult older teen\n",
            "6     updat charg approxim three time turn anyth cho...\n",
            "7     may fool soft inner pad skate terribl boot sid...\n",
            "8     purchas two put dog cute later notic fall apar...\n",
            "9     miss hardwar piec fit togeth like thi wa garba...\n",
            "10    look thi probabl fault thought purchas bear ba...\n",
            "11    want love thi u bought tv show work love first...\n",
            "12                   wa think wa go much smaller appear\n",
            "13    bought thi small gift scout friend like tanto ...\n",
            "14    water bottl silicon coat smell strongli like p...\n",
            "15                                            true size\n",
            "16    thi advertis one full quart weight onli weigh ...\n",
            "17    post reserv reviewaft experi buy black one hus...\n",
            "18    bought thi five year old swim like fish huge m...\n",
            "19    wast wa excit buy high qualiti umbrella use th...\n",
            "Name: cleaned_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline 3: Limpieza de Texto para Modelos Contextuales"
      ],
      "metadata": {
        "id": "-_83_qbE7em4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(clean_text_for_contextual_models(\"I bought 2 of these for my hiking trip for $5.99 each. They're amazing!\")) # Ejemplo de uso (prueba)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMACeivok03D",
        "outputId": "ea6d6e03-a0e9-4c92-f03f-cbaae1a1a117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bought two hiking trip five hundred and ninety-nine amazing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline 4: Función de limpieza que excluye los números"
      ],
      "metadata": {
        "id": "VcSa12qmDIDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(clean_text_exclude_numbers(\"I bought 2 of these for my hiking trip for $5.99 each. They're amazing!\")) # Ejemplo de uso (prueba)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZDeXsoeDTfz",
        "outputId": "2992b2ec-e171-4754-b064-787474c2e358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bought hiking trip amazing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reemplazar las columnas existentes con los nuevos textos limpios\n",
        "df['cleaned_text'] = df['text'].apply(clean_text_for_contextual_models)\n",
        "df['cleaned_text_exclude_numbers'] = df['text'].apply(clean_text_exclude_numbers)"
      ],
      "metadata": {
        "id": "oiSYFdKolp9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['text','cleaned_text', 'cleaned_text_exclude_numbers']].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU8pJCcqluuE",
        "outputId": "3d7cd8d7-bf46-4e86-e9de-e08c15e17d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  The travel mug was OK, not bad but the coffee ...   \n",
            "1  I'm terrible with planks so I was terrible at ...   \n",
            "2  Drawstring immediately ripped inches of stitch...   \n",
            "3  The black face is cracking and falling off aft...   \n",
            "4         Leaves white stuff in your hair. Not good!   \n",
            "5  Too large and not for little girls. More for a...   \n",
            "6  Update... after charging approximately 3 times...   \n",
            "7  you may be fooled by soft inner padding, but t...   \n",
            "8  I purchased two of these.  I put them on my do...   \n",
            "9  Missing hardware and pieces didn't fit togethe...   \n",
            "\n",
            "                                        cleaned_text  \\\n",
            "0  travel mug ok bad coffee mug team emblem affix...   \n",
            "1  terrible planks terrible maybe abs need lot st...   \n",
            "2  drawstring immediately ripped inches stitches ...   \n",
            "3  black face cracking falling two days poor quality   \n",
            "4                       leaves white stuff hair good   \n",
            "5              large little girls adults older teens   \n",
            "6  update charging approximately three times turn...   \n",
            "7  may fooled soft inner padding skate terrible b...   \n",
            "8  purchased two put dogs cute later noticed fall...   \n",
            "9  missing hardware pieces fit together like thin...   \n",
            "\n",
            "                        cleaned_text_exclude_numbers  \n",
            "0  travel mug ok bad coffee mug team emblem affix...  \n",
            "1  terrible plank terrible maybe ab need lot stre...  \n",
            "2  drawstring immediately ripped inch stitch prod...  \n",
            "3       black face cracking falling day poor quality  \n",
            "4                         leaf white stuff hair good  \n",
            "5                 large little girl adult older teen  \n",
            "6  update charging approximately time turn anythi...  \n",
            "7  may fooled soft inner padding skate terrible b...  \n",
            "8  purchased two put dog cute later noticed falli...  \n",
            "9  missing hardware piece fit together like thing...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6npjsHdFfB7",
        "outputId": "99d53b41-2d81-47f6-f02e-9327475dc52c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 14 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   rating                        50000 non-null  float64\n",
            " 1   title                         50000 non-null  object \n",
            " 2   text                          50000 non-null  object \n",
            " 3   images                        50000 non-null  object \n",
            " 4   asin                          50000 non-null  object \n",
            " 5   parent_asin                   50000 non-null  object \n",
            " 6   user_id                       50000 non-null  object \n",
            " 7   timestamp                     50000 non-null  int64  \n",
            " 8   helpful_vote                  50000 non-null  int64  \n",
            " 9   verified_purchase             50000 non-null  bool   \n",
            " 10  sentiment                     50000 non-null  object \n",
            " 11  cleaned_text                  50000 non-null  object \n",
            " 12  cleaned_text_exclude_numbers  50000 non-null  object \n",
            " 13  text_length                   50000 non-null  int64  \n",
            "dtypes: bool(1), float64(1), int64(3), object(9)\n",
            "memory usage: 5.0+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Guardar el DataFrame Modificado"
      ],
      "metadata": {
        "id": "3JjeY1Vgr0n7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save_data_drive(df,'processed_data.csv')  # En Google Drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VlOQ3Gq8FEd",
        "outputId": "a07873e5-c44f-44e2-d35a-a432b03f57e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame guardado en: /content/drive/My Drive/Colab Notebooks/Sports_and_Outdoors/cleaned_reviews.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_data_local(df, filename):\n",
        "    \"\"\"\n",
        "    Guarda un DataFrame en un archivo CSV localmente.\n",
        "\n",
        "    Parámetros:\n",
        "    - df: DataFrame a guardar.\n",
        "    - filename: Nombre del archivo para guardar el DataFrame.\n",
        "    \"\"\"\n",
        "    df.to_csv(filename, index=False, encoding='utf-8')\n",
        "    print(f\"DataFrame guardado localmente como: {filename}\")"
      ],
      "metadata": {
        "id": "kvmTUuFgPkEB"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar como CSV\n",
        "save_data_local(df,'processed_data.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LCs0liOEXrj",
        "outputId": "d6afff3a-ddfe-415b-f9bf-8d6fac814672"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame guardado localmente como: processed_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import shutil\n",
        "\n",
        "with open('processed_data.csv', 'rb') as f_in:\n",
        "    with gzip.open('processed_data.csv.gz', 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "f_out.close()\n",
        "f_in.close()"
      ],
      "metadata": {
        "id": "bkM0_xFYFzl6"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Durante el proceso de preprocesamiento y almacenamiento de los datos en un archivo CSV, se encontraron valores nulos inesperados al recargar el DataFrame en el archivo `3-entrenamiento-y-testeo-modelo`. Este problema no ocurrió al utilizar el formato Parquet para guardar y cargar los datos.\n",
        "\n",
        "La diferencia en el comportamiento se debe a cómo cada formato maneja la serialización y deserialización de los datos. **CSV** es un formato de texto simple que puede interpretar incorrectamente ciertos caracteres especiales o estructuras de datos complejas, lo que lleva a la pérdida o corrupción de información. **Parquet**, por otro lado, es un formato binario columnar optimizado para datos tabulares, que preserva de manera más efectiva los tipos de datos y la integridad de la información, evitando así los problemas observados con el formato CSV.\n",
        "\n",
        "Este hallazgo subraya la importancia de elegir el formato adecuado para el almacenamiento de datos, especialmente cuando se trabaja con texto y estructuras de datos complejas."
      ],
      "metadata": {
        "id": "VKaYJ_31Q8E6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_csv = pd.read_csv('processed_data.csv', encoding='utf-8'). # Cargar CSV\n",
        "\n",
        "# Verificar nuevamente valores nulos\n",
        "print(data_csv.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjqhCoOKO6aE",
        "outputId": "a15b6420-ffb7-470d-ea7d-7118389a7c99"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rating                           0\n",
            "title                            1\n",
            "text                             2\n",
            "images                           0\n",
            "asin                             0\n",
            "parent_asin                      0\n",
            "user_id                          0\n",
            "timestamp                        0\n",
            "helpful_vote                     0\n",
            "verified_purchase                0\n",
            "sentiment                        0\n",
            "cleaned_text                    80\n",
            "cleaned_text_exclude_numbers    82\n",
            "text_length                      0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar como Parquet\n",
        "df.to_parquet('processed_data.parquet')\n",
        "\n",
        "# Cargar Parquet\n",
        "data_parquet = pd.read_parquet('processed_data.parquet')\n",
        "\n",
        "# Verificar nuevamente valores nulos\n",
        "print(data_parquet.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2sj_VoqP85q",
        "outputId": "5183f16e-a680-4b9b-e8cc-aed91ecdca3f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rating                          0\n",
            "title                           0\n",
            "text                            0\n",
            "images                          0\n",
            "asin                            0\n",
            "parent_asin                     0\n",
            "user_id                         0\n",
            "timestamp                       0\n",
            "helpful_vote                    0\n",
            "verified_purchase               0\n",
            "sentiment                       0\n",
            "cleaned_text                    0\n",
            "cleaned_text_exclude_numbers    0\n",
            "text_length                     0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline de procesamiento\n",
        "\n",
        "En este cuaderno Jupyter, se han explorado y desarrollado diversas técnicas de preprocesamiento de texto adaptadas a las necesidades específicas del proyecto. A lo largo del proceso, se han ajustado y optimizado las funciones de limpieza.\n",
        "\n",
        "Para facilitar la **reutilización y mejorar la organización del código**, se ha consolidado este trabajo en un **pipeline de preprocesamiento** bien documentado, ubicado en el archivo `preprocessing.py`. Este archivo contiene todas las funciones de preprocesamiento definidas de manera modular, lo que permite una integración sencilla y directa en futuros proyectos o etapas de análisis dentro de este mismo proyecto."
      ],
      "metadata": {
        "id": "cOh8rqNzs6KP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones\n",
        "\n",
        "1. **Filtrado Efectivo**: La estrategia de eliminar stopwords y palabras de bajo valor semántico se añade con el objetivo de enfocar el análisis en el contenido más relevante. Sin embargo, es importante recalcar que la selección de estas palabras debe ser revisada continuamente para asegurar su pertinencia al contexto específico del análisis.\n",
        "\n",
        "2. **Uso de Beautiful Soup**: La decisión de integrar Beautiful Soup fue una respuesta directa a la presencia de elementos HTML no deseados y repetidos \"br\" identificados durante la fase exploratoria. A pesar de que esta inclusión puede impactar ligeramente la eficiencia, los beneficios en términos de la calidad del texto justifican su uso.\n",
        "\n",
        "3. **Manejo de Números con `num2words`**: La adopción de `num2words` enriquece el análisis al transformar los dígitos en texto, lo que facilita la exploración de patrones asociados a cantidades, precios o características numéricas en las reseñas.\n",
        "\n",
        "4. **Preservación del Contexto para Modelos Contextuales**: La decisión de minimizar las alteraciones agresivas en las formas de las palabras apunta a retener tanto el contexto como la semántica del texto.\n",
        "\n",
        "5. **Personalización y Flexibilidad**: Este enfoque proporciona una sólida base inicial para el preprocesamiento de texto. No obstante, es fundamental entenderlo como un punto de partida flexible, susceptible de ser adaptado o modificado en función de los requerimientos específicos del análisis o del conjunto de datos.\n",
        "\n",
        "6. **Balance entre Eficiencia y Efectividad**: Mientras que las primeras estrategias de preprocesamiento priorizaban la eficiencia al consolidar el vocabulario, la aproximación final destaca por su capacidad para capturar la complejidad semántica del texto, importante para análisis basados en el contexto.\n",
        "\n",
        "### Mejoras Futuras\n",
        "\n",
        "A pesar de que el enfoque actual se centra en la manipulación individual de palabras, es crucial reconocer las limitaciones inherentes a esta metodología, especialmente en lo que respecta a la captura de negaciones y expresiones idiomáticas. Futuras iteraciones podrían beneficiarse de incorporar técnicas que permitan un análisis más profundo del contexto y las relaciones entre palabras, posiblemente a través del uso de modelos de lenguaje avanzados.\n",
        "\n",
        "Además, es recomendable una evaluación continua y ajuste de los criterios de filtrado y limpieza para alinearlos estrechamente con las metas del proyecto y los hallazgos emergentes del análisis exploratorio. Este enfoque iterativo y reflexivo no solo mejora la precisión del análisis, sino que también garantiza la relevancia y actualidad del proceso de preprocesamiento."
      ],
      "metadata": {
        "id": "CPMStQPoodDr"
      }
    }
  ]
}